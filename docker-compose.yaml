services:
  trigger:
    image: 'ghcr.io/triggerdotdev/trigger.dev:main'
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      electric:
        condition: service_healthy
      minio:
        condition: service_healthy
      registry:
        condition: service_healthy
    user: root
    command: "sh -c \"chown -R node:node /home/node/shared && exec ./scripts/entrypoint.sh\"\n"
    environment:
      RUN_ENGINE_TASKS_PER_WORKER: 50
      RUN_ENGINE_WORKER_COUNT: 20
      DATABASE_CONNECTION_LIMIT: 200
      DEFAULT_ENV_EXECUTION_CONCURRENCY_LIMIT: 1000
      DEFAULT_ORG_EXECUTION_CONCURRENCY_LIMIT: 3000
      WORKER_CONCURRENCY: 50
      MAXIMUM_DEV_QUEUE_SIZE: 30000
      MAXIMUM_DEPLOYED_QUEUE_SIZE: 150000
      REMIX_APP_PORT: 3000
      APP_ORIGIN: ${SERVICE_URL_TRIGGER}
      LOGIN_ORIGIN: ${SERVICE_URL_TRIGGER}
      API_ORIGIN: ${SERVICE_URL_TRIGGER}
      SESSION_SECRET: '${SERVICE_PASSWORD_SESSION}'
      MAGIC_LINK_SECRET: '${SERVICE_PASSWORD_MAGIC}'
      ENCRYPTION_KEY: '${SERVICE_PASSWORD_ENCRYPTION}'
      MANAGED_WORKER_SECRET: '${SERVICE_PASSWORD_MANAGEDWORKER}'
      DATABASE_URL: 'postgresql://postgres:${SERVICE_PASSWORD_POSTGRES}@postgres:5432/${POSTGRES_DB:-trigger}?schema=public&sslmode=disable'
      DIRECT_URL: 'postgresql://postgres:${SERVICE_PASSWORD_POSTGRES}@postgres:5432/${POSTGRES_DB:-trigger}?schema=public&sslmode=disable'
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_TLS_DISABLED: true
      ELECTRIC_ORIGIN: 'http://electric:3000'
      DEV_OTEL_EXPORTER_OTLP_ENDPOINT: '${SERVICE_URL_TRIGGER}/otel'
      DEPLOY_REGISTRY_HOST: '${SERVICE_FQDN_REGISTRY}'
      DEPLOY_REGISTRY_NAMESPACE: '${REGISTRY_NAMESPACE:-trigger}'
      DEPLOY_REGISTRY_USERNAME: ${REGISTRY_USERNAME:-trigger}
      DEPLOY_REGISTRY_PASSWORD: '${SERVICE_PASSWORD_REGISTRY}'
      OBJECT_STORE_BASE_URL: 'http://minio:9000'
      OBJECT_STORE_ACCESS_KEY_ID: admin
      OBJECT_STORE_SECRET_ACCESS_KEY: '${SERVICE_PASSWORD_MINIO}'
      GRACEFUL_SHUTDOWN_TIMEOUT: 30000
      NODE_MAX_OLD_SPACE_SIZE: ${NODE_MAX_OLD_SPACE_SIZE:-8192}
      TRIGGER_BOOTSTRAP_ENABLED: 1
      TRIGGER_BOOTSTRAP_WORKER_GROUP_NAME: bootstrap
      TRIGGER_BOOTSTRAP_WORKER_TOKEN_PATH: /home/node/shared/worker_token
      CLICKHOUSE_URL: 'http://${CLICKHOUSE_ADMIN_USER:-default}:${SERVICE_PASSWORD_64_CLICKHOUSE}@clickhouse:8123?secure=false'
      CLICKHOUSE_LOG_LEVEL: info
      INTERNAL_OTEL_TRACE_LOGGING_ENABLED: 0
      RUN_REPLICATION_ENABLED: 1
      RUN_REPLICATION_CLICKHOUSE_URL: 'http://${CLICKHOUSE_ADMIN_USER:-default}:${SERVICE_PASSWORD_64_CLICKHOUSE}@clickhouse:8123'
      RUN_REPLICATION_LOG_LEVEL: info
      APP_LOG_LEVEL: info
      TRIGGER_TELEMETRY_DISABLED: 0
      # Email configuration (optional)
      WHITELISTED_EMAILS: '${WHITELISTED_EMAILS:-}'
      EMAIL_TRANSPORT: '${EMAIL_TRANSPORT:-}'
      FROM_EMAIL: '${FROM_EMAIL:-}'
      RESEND_API_KEY: '${RESEND_API_KEY:-}'
    volumes:
      - 'shared-data:/home/node/shared'
    healthcheck:
      test:
        - CMD
        - node
        - '-e'
        - "require('http').get('http://127.0.0.1:3000/healthcheck',(r)=>process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))"
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
  postgres:
    image: postgres:14
    restart: unless-stopped
    command:
      - -c
      - wal_level=logical
      - -c
      - max_connections=1000
      - -c
      - shared_buffers=512MB
      - -c
      - effective_cache_size=2GB
      - -c
      - work_mem=32MB
      - -c
      - maintenance_work_mem=256MB
      - -c
      - checkpoint_completion_target=0.9
    environment:
      POSTGRES_USER: 'postgres'
      POSTGRES_PASSWORD: '${SERVICE_PASSWORD_POSTGRES}'
      POSTGRES_DB: '${POSTGRES_DB:-trigger}'
    volumes:
      - 'postgres-data:/var/lib/postgresql/data'
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 5s
      timeout: 20s
      retries: 10
      start_period: 10s
  redis:
    image: 'redis:7'
    restart: unless-stopped
    command: redis-server --maxmemory 4gb --maxmemory-policy noeviction --appendonly yes --tcp-keepalive 300
    volumes:
      - 'redis-data:/data'
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
  electric:
    image: 'electricsql/electric:1.0.24'
    restart: unless-stopped
    depends_on:
      - postgres
    environment:
      DATABASE_URL: 'postgresql://postgres:${SERVICE_PASSWORD_POSTGRES}@postgres:5432/${POSTGRES_DB:-trigger}?schema=public&sslmode=disable'
      ELECTRIC_INSECURE: 'true'
      ELECTRIC_USAGE_REPORTING: 'false'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/v1/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
  clickhouse:
    image: 'bitnamilegacy/clickhouse:latest'
    restart: unless-stopped
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /bitnami/clickhouse/etc/config.d
        cat > /bitnami/clickhouse/etc/config.d/override.xml << 'XMLEOF'
        <clickhouse>
            <listen_host>0.0.0.0</listen_host>
            <logger><level>warning</level></logger>
            <mark_cache_size>524288000</mark_cache_size>
            <concurrent_threads_soft_limit_num>1</concurrent_threads_soft_limit_num>
            <profiles>
                <default>
                <max_block_size>8192</max_block_size>
                <max_download_threads>1</max_download_threads>
                <input_format_parallel_parsing>0</input_format_parallel_parsing>
                <output_format_parallel_formatting>0</output_format_parallel_formatting>
                </default>
            </profiles>
        </clickhouse>
        XMLEOF
        exec /opt/bitnami/scripts/clickhouse/entrypoint.sh /opt/bitnami/scripts/clickhouse/run.sh
    environment:
      CLICKHOUSE_ADMIN_USER: ${CLICKHOUSE_ADMIN_USER:-default}
      CLICKHOUSE_ADMIN_PASSWORD: '${SERVICE_PASSWORD_64_CLICKHOUSE}'
    volumes:
      - 'clickhouse-data:/bitnami/clickhouse'
    healthcheck:
      test: ["CMD-SHELL", "bash -c '</dev/tcp/localhost/8123'"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
  registry:
    image: 'registry:2'
    restart: unless-stopped
    entrypoint: /bin/sh
    command:
      - -c
      - |
        apk add --no-cache apache2-utils
        mkdir -p /auth
        htpasswd -Bbn "$${REGISTRY_USERNAME}" "$${REGISTRY_PASSWORD}" > /auth/htpasswd
        exec /entrypoint.sh /etc/docker/registry/config.yml
    environment:
      REGISTRY_USERNAME: ${REGISTRY_USERNAME:-trigger}
      REGISTRY_PASSWORD: '${SERVICE_PASSWORD_REGISTRY}'
      REGISTRY_AUTH: htpasswd
      REGISTRY_AUTH_HTPASSWD_REALM: 'Registry Realm'
      REGISTRY_AUTH_HTPASSWD_PATH: /auth/htpasswd
      REGISTRY_HTTP_SECRET: '${SERVICE_PASSWORD_REGISTRY}'
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:5000/"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
  minio:
    image: bitnamilegacy/minio:latest
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: '${SERVICE_PASSWORD_MINIO}'
      MINIO_DEFAULT_BUCKETS: packets
      MINIO_BROWSER: 'on'
    volumes:
      - 'minio-data:/bitnami/minio/data'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s

  supervisor:
    image: 'ghcr.io/triggerdotdev/supervisor:v4-beta'
    restart: unless-stopped
    depends_on:
      - docker-proxy
      - trigger
    user: root
    command: "sh -c \"chown -R node:node /home/node/shared && exec /usr/bin/dumb-init -- pnpm run --filter supervisor start\"\n"
    environment:
      DOCKER_ENFORCE_MACHINE_PRESETS: false
      TRIGGER_DEQUEUE_MAX_CONSUMER_COUNT: 5
      TRIGGER_DEQUEUE_MAX_RUN_COUNT: 20
      TRIGGER_API_URL: http://trigger:3000
      OTEL_EXPORTER_OTLP_ENDPOINT: http://trigger:3000/otel
      TRIGGER_WORKER_TOKEN: 'file:///home/node/shared/worker_token'
      MANAGED_WORKER_SECRET: ${SERVICE_PASSWORD_MANAGEDWORKER}
      TRIGGER_WORKLOAD_API_DOMAIN: supervisor
      TRIGGER_WORKLOAD_API_PORT_EXTERNAL: 8020
      DOCKER_HOST: tcp://docker-proxy:2375
      DOCKER_RUNNER_NETWORKS: ${DOCKER_RUNNER_NETWORKS:-trigger-net}
      DOCKER_AUTOREMOVE_EXITED_CONTAINERS: 1
      DOCKER_REGISTRY_URL: ${SERVICE_URL_REGISTRY}
      DOCKER_REGISTRY_USERNAME: ${REGISTRY_USERNAME:-trigger}
      DOCKER_REGISTRY_PASSWORD: '${SERVICE_PASSWORD_REGISTRY}'
      DEBUG: 1
      ENFORCE_MACHINE_PRESETS: 1
      TRIGGER_DEQUEUE_INTERVAL_MS: 500
    volumes:
      - 'shared-data:/home/node/shared'
    healthcheck:
      test: ["CMD", "node", "-e", "http.get('http://localhost:8020/health', res => process.exit(res.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
  docker-proxy:
    image: 'tecnativa/docker-socket-proxy:latest'
    restart: unless-stopped
    volumes:
      - '/var/run/docker.sock:/var/run/docker.sock:ro'
    environment:
      LOG_LEVEL: info
      POST: 1
      CONTAINERS: 1
      IMAGES: 1
      INFO: 1
      NETWORKS: 1
    healthcheck:
      test: ["CMD", "nc", "-z", "127.0.0.1", "2375"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 5s

  # ============================================
  # BACKUP SERVICES (Optional - set BACKUP_ENABLED=true to enable)
  # ============================================
  postgres-backup:
    image: itbm/postgresql-backup-s3:latest
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      BACKUP_ENABLED: '${BACKUP_ENABLED:-false}'
      POSTGRES_HOST: postgres
      POSTGRES_DATABASE: '${POSTGRES_DB:-trigger}'
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: '${SERVICE_PASSWORD_POSTGRES}'
      # S3 Configuration
      S3_ENDPOINT: '${BACKUP_S3_ENDPOINT:-}'
      S3_BUCKET: '${BACKUP_S3_BUCKET:-}'
      S3_ACCESS_KEY_ID: '${BACKUP_S3_ACCESS_KEY_ID:-}'
      S3_SECRET_ACCESS_KEY: '${BACKUP_S3_SECRET_ACCESS_KEY:-}'
      S3_REGION: '${BACKUP_S3_REGION:-us-east-1}'
      S3_PREFIX: '${DOCKER_RUNNER_NETWORKS:-trigger-net}/postgres'
      # Schedule (default: daily at 2:00 AM)
      SCHEDULE: '${BACKUP_POSTGRES_SCHEDULE:-0 2 * * *}'
      # Retention (default: keep 60 days) - deletes backups older than this
      DELETE_OLDER_THAN: '${BACKUP_POSTGRES_KEEP_DAYS:-60} days ago'
    entrypoint: /bin/sh
    command:
      - -c
      - |
        if [ "$${BACKUP_ENABLED}" != "true" ]; then
          echo "Backups disabled (BACKUP_ENABLED != true). Sleeping indefinitely..."
          tail -f /dev/null
        fi
        exec sh run.sh
    healthcheck:
      test: ["CMD-SHELL", "true"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  redis-backup:
    image: alpine:3.19
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - 'redis-data:/data:ro'
    environment:
      BACKUP_ENABLED: '${BACKUP_ENABLED:-false}'
      REDIS_HOST: redis
      REDIS_PORT: 6379
      # S3 Configuration
      S3_ENDPOINT: '${BACKUP_S3_ENDPOINT:-}'
      S3_BUCKET: '${BACKUP_S3_BUCKET:-}'
      S3_ACCESS_KEY_ID: '${BACKUP_S3_ACCESS_KEY_ID:-}'
      S3_SECRET_ACCESS_KEY: '${BACKUP_S3_SECRET_ACCESS_KEY:-}'
      S3_REGION: '${BACKUP_S3_REGION:-us-east-1}'
      S3_PREFIX: '${DOCKER_RUNNER_NETWORKS:-trigger-net}/redis'
      # Schedule (default: daily at 3:00 AM)
      BACKUP_SCHEDULE: '${BACKUP_REDIS_SCHEDULE:-0 3 * * *}'
      # Retention (default: keep 60 days)
      BACKUP_KEEP_DAYS: '${BACKUP_REDIS_KEEP_DAYS:-60}'
    entrypoint: /bin/sh
    command:
      - -c
      - |
        if [ "$${BACKUP_ENABLED}" != "true" ]; then
          echo "Backups disabled (BACKUP_ENABLED != true). Sleeping indefinitely..."
          tail -f /dev/null
        fi
        apk add --no-cache aws-cli redis bash gzip
        # Create backup script inline (self-contained, no external dependencies)
        cat > /backup.sh << 'BACKUP_SCRIPT'
        #!/bin/sh
        set -e
        TIMESTAMP=$$(date +%Y%m%d_%H%M%S)
        BACKUP_FILE="redis_backup_$${TIMESTAMP}.rdb"
        LOCAL_BACKUP_DIR="/tmp/redis-backups"
        mkdir -p "$${LOCAL_BACKUP_DIR}"
        echo "[$$(date)] Starting Redis backup..."
        # Save initial LASTSAVE timestamp
        INITIAL_LASTSAVE=$$(redis-cli -h "$${REDIS_HOST}" -p "$${REDIS_PORT}" LASTSAVE)
        # Trigger BGSAVE
        redis-cli -h "$${REDIS_HOST}" -p "$${REDIS_PORT}" BGSAVE
        echo "[$$(date)] Waiting for BGSAVE to complete..."
        # Wait for LASTSAVE to change (indicating BGSAVE completed)
        while [ "$$(redis-cli -h "$${REDIS_HOST}" -p "$${REDIS_PORT}" LASTSAVE)" = "$$INITIAL_LASTSAVE" ]; do
          sleep 1
        done
        sleep 2
        # Copy and compress the RDB file
        if [ -f /data/dump.rdb ]; then
          cp /data/dump.rdb "$${LOCAL_BACKUP_DIR}/$${BACKUP_FILE}"
          gzip "$${LOCAL_BACKUP_DIR}/$${BACKUP_FILE}"
          BACKUP_FILE="$${BACKUP_FILE}.gz"
          echo "[$$(date)] Backup compressed: $${BACKUP_FILE}"
        else
          echo "[$$(date)] ERROR: dump.rdb not found"
          exit 1
        fi
        # Configure AWS CLI
        export AWS_ACCESS_KEY_ID="$${S3_ACCESS_KEY_ID}"
        export AWS_SECRET_ACCESS_KEY="$${S3_SECRET_ACCESS_KEY}"
        export AWS_DEFAULT_REGION="$${S3_REGION:-us-east-1}"
        S3_ENDPOINT_URL=""
        if [ -n "$${S3_ENDPOINT:-}" ]; then
          S3_ENDPOINT_URL="--endpoint-url $${S3_ENDPOINT}"
        fi
        # Upload to S3
        S3_PATH="s3://$${S3_BUCKET}/$${S3_PREFIX}/$${BACKUP_FILE}"
        echo "[$$(date)] Uploading to $${S3_PATH}..."
        aws s3 cp $${S3_ENDPOINT_URL} "$${LOCAL_BACKUP_DIR}/$${BACKUP_FILE}" "$${S3_PATH}"
        echo "[$$(date)] Upload complete!"
        # Clean up old backups
        if [ -n "$${BACKUP_KEEP_DAYS:-}" ] && [ "$${BACKUP_KEEP_DAYS}" -gt 0 ]; then
          echo "[$$(date)] Cleaning up backups older than $${BACKUP_KEEP_DAYS} days..."
          CUTOFF_DATE=$$(date -d "-$${BACKUP_KEEP_DAYS} days" +%Y%m%d 2>/dev/null || date -v-$${BACKUP_KEEP_DAYS}d +%Y%m%d)
          aws s3 ls $${S3_ENDPOINT_URL} "s3://$${S3_BUCKET}/$${S3_PREFIX}/" | while read -r line; do
            FILE_NAME=$$(echo "$$line" | awk '{print $$4}')
            if [ -n "$$FILE_NAME" ]; then
              FILE_DATE=$$(echo "$$FILE_NAME" | sed -n 's/redis_backup_\([0-9]\{8\}\)_.*/\1/p')
              if [ -n "$$FILE_DATE" ] && [ "$$FILE_DATE" -lt "$$CUTOFF_DATE" ]; then
                echo "[$$(date)] Deleting old backup: $${FILE_NAME}"
                aws s3 rm $${S3_ENDPOINT_URL} "s3://$${S3_BUCKET}/$${S3_PREFIX}/$${FILE_NAME}"
              fi
            fi
          done
        fi
        rm -f "$${LOCAL_BACKUP_DIR}/$${BACKUP_FILE}"
        echo "[$$(date)] Redis backup completed successfully!"
        BACKUP_SCRIPT
        chmod +x /backup.sh
        printf '%s /backup.sh\n' "$${BACKUP_SCHEDULE}" > /etc/crontabs/root
        crond -f -l 2
    healthcheck:
      test: ["CMD-SHELL", "true"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

volumes:
  postgres-data:
  redis-data:
  clickhouse-data:
  minio-data:
  shared-data:
  registry-data:
